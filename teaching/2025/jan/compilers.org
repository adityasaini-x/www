#+TITLE: Compilers

* Policy

Attend lectures only if you want to. You are expected to keep up to
date on the discussions that happen during lectures and labs.

** Evaluation

| Assessment            | Percentage | Remarks                     |
|-----------------------+------------+-----------------------------|
| Lab quizzes           |         30 | Small programming exercises |
| Semester-long project |         50 | Team of 4.                  |
| Mid-semester exam     |         10 |                             |
| End-semester exam     |         10 |                             |

** Resources

There is no textbook. Follow the lectures. Code a lot. Read necessary
theory.

[[https://craftinginterpreters.com/][Bob Nystrom's Crafting Interpreters]] is a good book. I will disagree
with this book on some of the topics.

* Lectures

** Calculator back-end

You have all used compilers and you know what they do. In this course,
we will build one and explore how compilers are organized and the
relevant algorithms.

The most common data structure in a compiler is a tree and the most
common algorithm, the post-order traversal. To see why, let us build a
simple calculator. It accepts expressions and evaluates them. What is
the best way to represent an expression like =2 + 3*5= in a computer?
#+begin_example

       +
      / \
     2   *
        / \
       3   5

#+end_example
Yes. A tree. How would you evaluate this expression? Evaluate the left
sub-tree, then the right sub-tree, then add the results. That is, a
post-order traversal.

The front-end of the compiler has to take the string =2 + 3*5= and
build this tree. This is done using two classes of algorihms called
lexing and parsing, which correspond to DFAs and CFGs respectively. We
will see this part later. First, we will build the back-end for the
compiler. It takes trees as input and evaluates them.

First, let us define some classes in Python to represent expression
trees.
#+begin_src python :session leroy :python ".compilers/.venv/bin/python" :results none
  from dataclasses import dataclass

  class AST:
      pass

  @dataclass
  class BinOp(AST):
      op: str
      left: AST
      right: AST

  @dataclass
  class Number(AST):
      val: str
#+end_src

The tree for the previous expression can be created so:
#+begin_src python :session leroy :results output
  expr_t1 = BinOp("+", Number("2"), BinOp("*", Number("3"), Number("5")))
  print(expr_t1)
#+end_src

#+RESULTS:
: BinOp(op='+', left=Number(val='2'), right=BinOp(op='*', left=Number(val='3'), right=Number(val='5')))

Now, let us write an evaluator. This function should take the tree as
input and produce a number.
#+begin_src python :session leroy :results none
  def e(tree: AST) -> int:
      match tree:
          case Number(v): return int(v)
          case BinOp("+", l, r): return e(l) + e(r)
          case BinOp("*", l, r): return e(l) * e(r)
#+end_src
Let us test it by evaluating our sample expression.
#+begin_src python :session leroy :results value
  e(expr_t1)
#+end_src

#+RESULTS:
: 17

Yay! Now, extend this small program to make what you think a real
calculator should be.

** Calculator front-end

The front-end of the calculator should take a string as input and
return the corresponding expression tree. We split this into a lexer
and a parser.

The lexer converts a stream of characters into a stream of
tokens. First, we define the tokens of our language.
#+begin_src python :session leroy :results none
  class Token:
      pass

  @dataclass
  class NumberToken(Token):
      v: str

  @dataclass
  class OperatorToken(Token):
      o: str
#+end_src

Now, the lexer can produce the token stream.
#+begin_src python :session leroy :results none
  from collections.abc import Iterator
  def lex(s: str) -> Iterator[Token]:
      i = 0
      while True:
          while i < len(s) and s[i].isspace():
              i = i + 1

          if i >= len(s):
              return

          if s[i].isdigit():
              t = s[i]
              i = i + 1
              while i < len(s) and s[i].isdigit():
                  t = t + s[i]
                  i = i + 1
              yield NumberToken(t)
          else:
              match t := s[i]:
                  case '+' | '*':
                      i = i + 1
                      yield OperatorToken(t)
#+end_src

Let us test our lexer with our sample expression.
#+begin_src python :session leroy :results output
  for t in lex("2 + 3*5"):
      print(t)
#+end_src

#+RESULTS:
: NumberToken(v='2')
: OperatorToken(o='+')
: NumberToken(v='3')
: OperatorToken(o='*')
: NumberToken(v='5')

The parser takes the token stream as input and produces the AST.
#+begin_src python :session leroy :results none
  from collections.abc import Iterator
  def parse(s: str) -> AST:
      from more_itertools import peekable
      t = peekable(lex(s))

      def parse_add():
          ast = parse_mul()
          while True:
              match t.peek(None):
                  case OperatorToken('+'):
                      next(t)
                      ast = BinOp('+', ast, parse_mul())
                  case _:
                      return ast

      def parse_mul():
          ast = parse_atom()
          while True:
              match t.peek(None):
                  case OperatorToken('*'):
                      next(t)
                      ast = BinOp("*", ast, parse_atom())
                  case _:
                      return ast

      def parse_atom():
          match t.peek(None):
              case NumberToken(v):
                  next(t)
                  return Number(v)

      return parse_add()
#+end_src

#+RESULTS:

Let us test the parser for some expressions.
#+begin_src python :session leroy :results output
  print(parse("2"))
  print(parse("2+3"))
  print(parse("2+3*5"))
#+end_src

#+RESULTS:
: Number(val='2')
: BinOp(op='+', left=Number(val='2'), right=Number(val='3'))
: BinOp(op='+', left=Number(val='2'), right=BinOp(op='*', left=Number(val='3'), right=Number(val='5')))

To put the front-end and back-end together, we just have to compose
~parse~ and ~e~.
#+begin_src python :session leroy :results output
  print(e(parse("2 + 3*5")))
#+end_src

#+RESULTS:
: 17

Now is the time to extend this into a more useful calculator. I
suggest adding the following features.
- All usual arithmetic operators, including the unary negation.
- Parenthesis for grouping.
- Use a proper number type for evaluation.
- Error-handling in the front-end.

** Conditional expressions

We will implement a conditional expression of the form ~if a then b
else c end~. To do this, we need the following additions:
- Lexing: Add support for keywords like ~if~.
- Parsing: Add support for conditional expressions. This has a lower
  precedence than arithmetic.
- Evaluation: Modify the post-order traversal to conditionally
  traverse sub-trees.

As before, we will start by defining the AST node.
#+begin_src python :results none :session leroy
  @dataclass
  class If(AST):
      c: AST
      t: AST
      e: AST
#+end_src
We will use ~BinOp~ for comparison operators.

We want to handle expressions like:
#+begin_src python :session leroy :results none
  expr_t2 = If(BinOp("<", Number("2"), Number("3")), Number("0"), Number("1"))
  expr_t3 = If(BinOp("<", Number("3"), Number("2")), Number("0"), Number("1"))
#+end_src

The evaluator is:
#+begin_src python :results none :session leroy
  def e(tree: AST) -> int | bool:
      match tree:
          case Number(v): return int(v)
          case BinOp("+", l, r): return e(l) + e(r)
          case BinOp("*", l, r): return e(l) * e(r)
          case BinOp("<", l, r): return e(l) < e(r)
          case If(cond, then, else_):
              if e(cond):
                  return e(then)
              else:
                  return e(else_)
#+end_src

Let us test it with our sample expression.
#+begin_src python :session leroy :results output :exports both
  print(e(expr_t2))
  print(e(expr_t3))
#+end_src

#+RESULTS:
: 0
: 1

Now, we will modify the front-end to allow the programmer to input
these expressions. We will need a token type for keywords.
#+begin_src python :results none :session leroy
  @dataclass
  class KeywordToken(Token):
      w: str
#+end_src

In ~lex~, if the first character is a letter, then we will try and
form a keyword token. We should also handle comparison operators.
#+begin_src python :session leroy :results none
  from collections.abc import Iterator
  def lex(s: str) -> Iterator[Token]:
      i = 0
      while True:
          while i < len(s) and s[i].isspace():
              i = i + 1

          if i >= len(s):
              return

          if s[i].isalpha():
              t = s[i]
              i = i + 1
              while i < len(s) and s[i].isalpha():
                  t = t + s[i]
                  i = i + 1
              # XXX: Should check here whether we got a valid keyword.
              yield KeywordToken(t)
          elif s[i].isdigit():
              t = s[i]
              i = i + 1
              while i < len(s) and s[i].isdigit():
                  t = t + s[i]
                  i = i + 1
              yield NumberToken(t)
          else:
              match t := s[i]:
                  case '+' | '*' | '<':
                      i = i + 1
                      yield OperatorToken(t)
#+end_src

Let us see what tokens are produced by our new lexer.
#+begin_src python :results output :session leroy :exports both
  for t in lex("if 2 < 3 then 0 else 1 end"):
      print(t)
#+end_src

#+RESULTS:
: KeywordToken(w='if')
: NumberToken(v='2')
: OperatorToken(o='<')
: NumberToken(v='3')
: KeywordToken(w='then')
: NumberToken(v='0')
: KeywordToken(w='else')
: NumberToken(v='1')
: KeywordToken(w='end')

It looks fine. Now, we move on to parsing.

While parsing, we will peek at the first token. If it is the keyword
~if~, then we will try to parse an ~if~ expression. If it begins with
another token, we will try and parse an arithmetic expression.

#+begin_src python :session leroy :results none
  class ParseError(Exception):
      pass

  def parse(s: str) -> AST:
      from more_itertools import peekable
      t = peekable(lex(s))

      def expect(what: Token):
          if t.peek(None) == what:
              next(t)
              return
          raise ParseError

      def parse_if():
          match t.peek(None):
              case KeywordToken("if"):
                  next(t)
                  cond = parse_if()
                  expect(KeywordToken("then"))
                  then = parse_if()
                  expect(KeywordToken("else"))
                  else_ = parse_if()
                  expect(KeywordToken("end"))
                  return If(cond, then, else_)
              case _:
                  return parse_cmp()

      def parse_cmp():
          l = parse_add()
          if t.peek(None) == OperatorToken('<'):
              next(t)
              r = parse_add()
              return BinOp('<', l, r)
          else:
              return l

      def parse_add():
          ast = parse_mul()
          while True:
              match t.peek(None):
                  case OperatorToken('+'):
                      next(t)
                      ast = BinOp('+', ast, parse_mul())
                  case _:
                      return ast

      def parse_mul():
          ast = parse_atom()
          while True:
              match t.peek(None):
                  case OperatorToken('*'):
                      next(t)
                      ast = BinOp("*", ast, parse_atom())
                  case _:
                      return ast

      def parse_atom():
          match t.peek(None):
              case NumberToken(v):
                  next(t)
                  return Number(v)

      return parse_if()
#+end_src

Let us test it on a sample.
#+begin_src python :session leroy :results output :exports both
  print(parse("if 2 < 3 then 0+5 else 1*6 end"))
#+end_src

#+RESULTS:
: If(c=BinOp(op='<', left=Number(val='2'), right=Number(val='3')), t=BinOp(op='+', left=Number(val='0'), right=Number(val='5')), e=BinOp(op='*', left=Number(val='1'), right=Number(val='6')))

** Variables

Consider the following emacs lisp example:
#+begin_src emacs-lisp :results output :exports both
  (defvar v 1)
  
  (defun foo ()
    (princ v))

  (defun bar ()
    (let ((v 10))
      (foo)))

  (bar)
#+end_src

#+RESULTS:
: 10

Compare it with the following Python code that is the same except in
the scoping of variables.
#+begin_src python :python "python3" :results output :exports both
  v = 1

  def foo():
      print(v)

  def bar():
      v = 10
      foo()

  bar()
#+end_src

#+RESULTS:
: 1

The ~v~ in ~foo()~ refers to the global ~v~ in Python. The ~v~ in
~bar()~ is local to that function and cannot escape unless it is
passed as an argument to ~foo()~. This is called lexical scoping. The
association of variable use to declaration is determined by program
text. Emacs lisp uses dynamic scoping, where the association is
determined by the state of execution.

#+begin_src emacs-lisp :results output :exports both
  (defvar v 1)

  (defun foo ()
    (print v))

  (defun bar (a)
    (if a (let ((v 10)) (foo)) (foo)))

  (bar nil)
  (bar t)
#+end_src

#+RESULTS:
: 
: 1
: 
: 10

In the following example, observe that ~foo()~ picks up either the
global ~v~ or the ~v~ local to ~bar()~ depending on the argument to
~bar()~.

#+RESULTS:
: 
: 1
: 
: 10

Modern languages all use lexical scoping by default. But, an option to
have dynamic variables is also nice.

** Dynamic scoping

Implementing dynamic scoping is easy. During evaluation, we keep a
stack that maps variable names to values. Each variable declaration
pushes an entry into this stack and each variable use looks for the
variable in this stack from top to bottom. This will pick-up the most
recently declared variable of the same name.

If your language has no functions, then there is no difference between
lexical and dynamic scoping.

The following class represents the programming construct ~let v be e
in f end~ which binds the value ~e~ to ~v~ and then evaluates
~f~. Here, ~v~ must be a valid identifier and ~e~ and ~f~ must be
expressions.
#+begin_src python :session leroy :results none
  @dataclass
  class Let(AST):
      v: str
      e: AST
      f: AST
#+end_src

We also need a class for occurences of variables within expressions.
#+begin_src python :session leroy :results none
  @dataclass
  class Var(AST):
      v: str
#+end_src

The evaluation now requires a stack in addition to the AST. Usually,
this stack is called the environment.
#+begin_src python :session leroy :results none
  def lookup(env, v):
      for u, uv in reversed(env):
          if u == v:
              return uv
      raise ValueError("No value found.")

  def e(t: AST, env = None) -> int:
      if env is None: env = []

      match t:
          case Var(v):
              return lookup(env, v)
          case Let(v, x, y):
              vv = e(x, env)
              env.append((v, vv))
              yv = e(y, env)
              env.pop()
              return yv
          case Number(s):
              return int(s)
          case BinOp("+", l, r):
              return  e(l, env) + e(r, env)
      assert False
#+end_src

Let us create two sample ASTs and test them.
#+begin_src python :session leroy :results none
  expr_t4 = Let("a", Number("3"), BinOp("+", Var("a"), Var("a")))
  expr_t5 = Let("a", Number("3"),
		Let("b", BinOp("+", Var("a"), Number("2")),
		    BinOp("+", Var("a"), Var("b"))))
#+end_src

#+begin_src python :session leroy :results output :exports both
  print(e(expr_t4))
  print(e(expr_t5))
#+end_src

#+RESULTS:
: 6
: 8

Now, we will add the surface syntax which will look like ~let a be 3
in a + a end~. I prefer keywords such as ~be~ instead of symbols such
as ~=~ or ~:=~ as words are easier to type for me. However, when
choosing a keyword, we have to ensure that it is a word that will not
be used often as variable names. The word ~be~ is not a common
variable name. So this choice is fine.

The lexer should have variable tokens.
#+begin_src python :session leroy :results none
  @dataclass
  class VarToken(Token):
      v: str
#+end_src

After lexing a word, we have to lookup whether it is a keyword. If it
is, then we return a keyword token. Otherwise, it is a variable.
#+begin_src python :session leroy :results none
  from collections.abc import Iterator

  def lex(s: str) -> Iterator[Token]:
      i = 0
      while True:
          while i < len(s) and s[i].isspace():
              i = i + 1

          if i >= len(s):
              return

          if s[i].isalpha():
              t = s[i]
              i = i + 1
              while i < len(s) and s[i].isalpha():
                  t = t + s[i]
                  i = i + 1
              if t in { "let", "be", "in", "end" }:
                  yield KeywordToken(t)
              else:
                  yield VarToken(t)
          elif s[i].isdigit():
              t = s[i]
              i = i + 1
              while i < len(s) and s[i].isdigit():
                  t = t + s[i]
                  i = i + 1
              yield NumberToken(t)
          else:
              match t := s[i]:
                  case '+' | '*' | '<':
                      i = i + 1
                      yield OperatorToken(t)
#+end_src

Let us test the lexer.
#+begin_src python :session leroy :results output :exports both
  for t in lex("let a be 3 in a + a end"):
      print(t)
#+end_src

#+RESULTS:
: KeywordToken(w='let')
: VarToken(v='a')
: KeywordToken(w='be')
: NumberToken(v='3')
: KeywordToken(w='in')
: VarToken(v='a')
: OperatorToken(o='+')
: VarToken(v='a')
: KeywordToken(w='end')

The parser tries to parse a ~let~ expression if it observes the token
~let~ at the beginning.
#+begin_src python :session leroy :results none
  class ParseError(Exception):
      msg: str

  def parse(s: str) -> AST:
      from more_itertools import peekable
      t = peekable(lex(s))

      def expect(what: Token):
          if t.peek(None) == what:
              next(t)
              return
          raise ParseError(f"Expected {what}")

      def parse_let():
          match t.peek(None):
              case KeywordToken("let"):
                  next(t)
                  vt = next(t)
                  expect(KeywordToken("be"))
                  e = parse_let()
                  expect(KeywordToken("in"))
                  f = parse_let()
                  expect(KeywordToken("end"))
                  return Let(vt.v, e, f)
              case _:
                  return parse_add()

      def parse_add():
          ast = parse_atom()
          while True:
              match t.peek(None):
                  case OperatorToken('+'):
                      next(t)
                      ast = BinOp('+', ast, parse_atom())
                  case _:
                      return ast

      def parse_atom():
          match t.peek(None):
              case NumberToken(v):
                  next(t)
                  return Number(v)
              case VarToken(v):
                  next(t)
                  return Var(v)

      return parse_let()
#+end_src

#+begin_src python :session leroy :results output :exports both
  print(e(parse("let a be 3 in a + a end")))
  print(e(parse("let a be 3 in let b be a + 2 in a + b end end")))
#+end_src

#+RESULTS:
: 6
: 8
